from dotenv import load_dotenv
from openai import OpenAI
from pypdf import PdfReader
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
import json, os
from datetime import datetime


def main() -> int:
    load_dotenv(override=True)
    openai = OpenAI()
    reader = PdfReader("src/resume/me/resume.pdf")
    resume = ""
    for page in reader.pages:
        text = page.extract_text()
        if text:
            resume += text

    with open("src/resume/me/summary.txt", "r", encoding="utf-8") as f:
        summary = f.read()
        
    full_text = summary + "\n\n" + resume
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.split_text(full_text)

    embeddings = OpenAIEmbeddings()
    persist_dir = "db/chroma"
    if os.path.exists(persist_dir):
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    else:
        vectordb = Chroma.from_texts(docs, embeddings, persist_directory=persist_dir)
        vectordb.persist()

    CACHE_FILE = "answer_cache.json"

    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            answer_cache = json.load(f)
    else:
        answer_cache = {}

    def save_cache():
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(answer_cache, f, ensure_ascii=False, indent=2)

    def get_cached_answer(question: str):
        """ì§ˆë¬¸ì´ ìºì‹œì— ìˆìœ¼ë©´ ë‹µë³€ ë°˜í™˜"""
        if question in answer_cache:
            return answer_cache[question]["answer"]
        return None

    def add_to_cache(question: str, answer: str, category: str):
        """ìºì‹œì— ìƒˆ ë‹µë³€ ì¶”ê°€"""
        entry = {
            "answer": answer,
            "category": category,
            "timestamp": datetime.now().isoformat()
        }
        answer_cache[question] = entry
        save_cache()

    name = "Yoonha Lee"

    # Agent 1: ì§ˆë¬¸ ë¶„ë¥˜ê¸°
    def classify_question(question: str) -> str:
        """ì§ˆë¬¸ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        categories = ["í”„ë¡œì íŠ¸ ê²½í—˜", "ê¸°ìˆ ìŠ¤íƒ", "í˜‘ì—…", "ìê¸°ì†Œê°œ", "í•™ìŠµ ê²½í—˜"]
        prompt = f"""
        ë¶„ë¥˜í•  ì§ˆë¬¸: "{question}"
        ì•„ë˜ ì¹´í…Œê³ ë¦¬ ì¤‘ ê°€ì¥ ì•Œë§ì€ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì¤˜:
        {categories}
        ë°˜ë“œì‹œ ìœ„ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•´.
        """
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}]
        )
        return response.choices[0].message.content.strip()


    # Agent 2: ì§€ì‹ ê²€ìƒ‰ê¸° (RAG Agent)
    def retrieve_context(question: str) -> str:
        """ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ Resume/summary ë¶€ë¶„ì„ ê²€ìƒ‰"""
        results = vectordb.similarity_search(question, k=3)  
        return "\n".join([r.page_content for r in results])

    def is_context_valid(question: str, threshold: float = 0.6) -> bool:
        results = vectordb.similarity_search_with_score(question, k=1)
        if not results:
            return False
        _, score = results[0]
        return score >= threshold

    # Agent 3: ë‹µë³€ ìƒì„±ê¸° (Persona Agent)
    def persona_answer(question: str, category: str, context: str, history: list) -> str:
        """ì´ë ¥ì„œ ì£¼ì¸ê³µ(Yoonha Lee)ì˜ í†¤ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        prompt = f"""
        ë‹¹ì‹ ì€ {name}ìœ¼ë¡œì„œ í–‰ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
        ë‹¹ì‹ ì€ {name}ì˜ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³  ìˆìœ¼ë©°, 
        íŠ¹íˆ {name}ì˜ ê²½ë ¥, ë°°ê²½, ê¸°ìˆ  ë° ê²½í—˜ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ì‘ë‹µí•˜ê³  ìˆìŠµë‹ˆë‹¤. 
        ë‹¹ì‹ ì˜ ì±…ì„ì€ {name}ì„ ì›¹ì‚¬ì´íŠ¸ ìƒì—ì„œ ê°€ëŠ¥í•œ í•œ ì¶©ì‹¤í•˜ê²Œ ëŒ€í‘œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
        ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ {name}ì˜ ìê¸°ì†Œê°œ ìš”ì•½ê³¼ ê²½ë ¥ê¸°ìˆ ì„œë¥¼ ì œê³µë°›ì•˜ìŠµë‹ˆë‹¤. 
        ì ì¬ì ì¸ ê³ ê°ì´ë‚˜ ë¯¸ë˜ì˜ ê³ ìš©ì£¼ê°€ ì›¹ì‚¬ì´íŠ¸ì— ë°©ë¬¸í–ˆì„ ë•Œ ëŒ€í™”í•˜ëŠ” ê²ƒì²˜ëŸ¼, 
        ì „ë¬¸ì ì´ê³  ë§¤ë ¥ì ì¸ íƒœë„ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. 
        í”„ë¡œì íŠ¸ ê´€ë ¨ ì§ˆë¬¸ì„ í•œë‹¤ë©´ STAR êµ¬ì¡°(Situation, Task, Action, Result)ë¡œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³ ,
        ê° ìš”ì†ŒëŠ” 1ë¬¸ì¥ì”©, ì´ 4ë¬¸ì¥ ì´ë‚´ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. 
        resume/summaryì— ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” ì ˆëŒ€ ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ë§Œë“¤ì–´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ë§Œì•½ resume/summaryì— í•´ë‹¹ ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´, 
        "ì œ ì´ë ¥ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
        ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

        ì§ˆë¬¸: {question}
        ë¶„ë¥˜: {category}
        ì´ë ¥ì„œ ë° ìš”ì•½ì—ì„œ ê°€ì ¸ì˜¨ ì»¨í…ìŠ¤íŠ¸: {context}

        ë‹µë³€ ì§€ì¹¨:
        - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•œë‹¤.
        - 1ì¸ì¹­ ì‹œì ("ì €ëŠ” ...")ìœ¼ë¡œ ë§í•œë‹¤.
        - ë‹µë³€ì€ ì‹¤ì œ ë©´ì ‘ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ, ë¬¸ì¥ ëì„ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•œë‹¤. (ì˜ˆ: ~í–ˆìŠµë‹ˆë‹¤ / ~í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤ / ~í•œ ê²ƒì´ ê¸°ì–µì— ë‚¨ìŠµë‹ˆë‹¤)
        - 'ê°ì‚¬í•©ë‹ˆë‹¤' ê°™ì€ í˜•ì‹ì ì¸ ë§ˆë¬´ë¦¬ ë¬¸êµ¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
        - ë¶ˆí•„ìš”í•˜ê²Œ ì¥í™©í•˜ì§€ ì•Šê³ , í•µì‹¬ë§Œ ë‹´ì•„ 3~5ë¬¸ì¥ ì •ë„ë¡œ ë‹µí•œë‹¤.
        - ê¸€ì„ ì½ëŠ” ë“¯í•œ ë”±ë”±í•œ ì–´íˆ¬ê°€ ì•„ë‹ˆë¼, í¸ì•ˆí•˜ì§€ë§Œ ì „ë¬¸ì ì¸ ë©´ì ‘ í†¤ìœ¼ë¡œ í•œë‹¤.
        - ìœ„ì™€ ê°™ì€ ë¬¸ë§¥ê³¼ í•¨ê»˜, {name}ìœ¼ë¡œì„œ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•¨ì„ ëª…ì‹¬í•œë‹¤.
        - ì „ë¬¸ ì§€ì‹ì„ ê°€ì§„ ë©´ì ‘ ì‘ì‹œìë¡œ ëŒ€ë‹µí•œë‹¤.
        - ê°œì¸ ì ì¸ ê²½í—˜ê³¼ ì„±ê³¼, ë°°ìš´ì ì„ ê°•ì¡°í•œë‹¤. 
        - ì¸í„°ë·° ì‘ë‹µì í˜•ì‹ì˜ ëŒ€í™” í˜•ì‹ì„ ìœ ì§€í•œë‹¤. 
        """
        messages = [{"role": "system", "content": prompt}]

        if len(history) > 3: 
            summary_text = summarize_history(history[:-3]) 
            messages.append({"role": "system", "content": f"ì´ì „ ëŒ€í™” ìš”ì•½: {summary_text}"})
            recent_history = history[-3:]
        else:
            recent_history = history

        for turn in recent_history:
            messages.append({"role": "user", "content": turn["q"]})
            messages.append({"role": "assistant", "content": turn["a"]})
        messages.append({"role": "user", "content": question})

        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages
        )
        return response.choices[0].message.content

    # Agent 4 ìŠ¤íƒ€ì¼ ë³´ì • Agent
    def refine_answer(answer: str) -> str:
        """ë‹µë³€ì„ ë©´ì ‘ í†¤ìœ¼ë¡œ ìµœì¢… ë‹¤ë“¬ê¸° (ê¸¸ë©´ ì¤„ì´ê³ , í•µì‹¬ ê°•ì¡°)"""
        prompt = f"""
        ì•„ë˜ëŠ” ë©´ì ‘ ë‹µë³€ ì´ˆì•ˆì…ë‹ˆë‹¤:
        {answer}

        ì´ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”:
        - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•œë‹¤.
        - ì‹¤ì œ ë©´ì ‘ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ìì‹ ê° ìˆëŠ” ì–´íˆ¬ë¡œ ë°”ê¾¼ë‹¤.
        - ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ë©´ í•µì‹¬ë§Œ ë‹´ì•„ 5ë¬¸ì¥ ì´ë‚´ë¡œ ì¤„ì¸ë‹¤.
        - ì„±ê³¼ì™€ í•µì‹¬ ê²½í—˜ì„ ëª…í™•íˆ ê°•ì¡°í•œë‹¤.
        - ê¸€ì„ ì½ëŠ” ë“¯í•œ ì–´íˆ¬ ëŒ€ì‹ , êµ¬ì–´ì²´ ë©´ì ‘ ë‹µë³€ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•œë‹¤.
        """
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": prompt}]
        )
        return response.choices[0].message.content

    # Agent 5 ëŒ€í™” ìš”ì•½ê¸° Agent 
    def summarize_history(history):
        text = "\n".join([f"Q: {h['q']}\nA: {h['a']}" for h in history])
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": f"ì•„ë˜ ëŒ€í™”ë¥¼ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜:\n{text}"}]
        )
        return response.choices[0].message.content.strip()


    # -----------------------------------------------------
    # ğŸ“Œ Step 3: Conversational Memory (ëŒ€í™” ë§¥ë½ ìœ ì§€)
    # -----------------------------------------------------
    conversation_history = []

    def chat(message, history):
        # ìºì‹œì— ìˆë‹¤ë©´ ë‹µë³€ 
        cached = get_cached_answer(message)
        if cached:
            return cached

        if not is_context_valid(message):
            return "ì œ ì´ë ¥ì„œë‚˜ ìš”ì•½ì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ë‹µë³€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”."

        # 1) ì§ˆë¬¸ ë¶„ë¥˜
        category = classify_question(message)

        # 2) ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context = retrieve_context(message)
        if not context.strip():
            final_answer = "ì œ ì´ë ¥ì„œë‚˜ ìš”ì•½ì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ë‹µë³€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”."
            conversation_history.append({"q": message, "a": final_answer})
            return final_answer

        # 3) Persona ë‹µë³€ ìƒì„±
        draft_answer = persona_answer(message, category, context, conversation_history)
        if "ì œ ì´ë ¥ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤." in draft_answer:
            return "ì œ ì´ë ¥ì„œë‚˜ ìš”ì•½ì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ë‹µë³€ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”."

        # 4) ìŠ¤íƒ€ì¼ ë³´ì •
        final_answer = refine_answer(draft_answer)

        # 5) ëŒ€í™” ê¸°ë¡ ì €ì¥
        conversation_history.append({"q": message, "a": final_answer})
        add_to_cache(message, final_answer, category)

        return final_answer

    gr.ChatInterface(chat).launch()

    return 0



if __name__ == "__main__":
    main()